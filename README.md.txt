# üß† CSE 574-D: Introduction to Machine Learning ‚Äì Assignment 1  
### **Classification and Regression Methods from Scratch**

---

## üìù Description
Implementation of core **machine learning algorithms** ‚Äî Logistic Regression, Linear Regression, and Ridge Regression ‚Äî from scratch using Python and NumPy.  
Includes full **data preprocessing**, **visualization**, and **model evaluation** for multiple datasets as part of *CSE 574: Introduction to Machine Learning (Fall 2023)*.

---

## üéØ Objective
This project focuses on understanding the mathematical foundation of machine learning by manually implementing classification and regression algorithms without external ML libraries such as scikit-learn. You will preprocess datasets, train models, and visualize performance metrics using Python.

---

## üìÇ Repository Structure


---

## ‚öôÔ∏è Technologies Used
- **Python 3.10+**
- **NumPy** ‚Äì mathematical computations  
- **Pandas** ‚Äì data manipulation  
- **Matplotlib / Seaborn** ‚Äì visualization  
- **Pickle** ‚Äì model serialization  
- **Jupyter Notebook** ‚Äì experimentation  

> üö´ ML libraries such as scikit-learn are not used ‚Äî everything is built manually.

---

## üß© Assignment Parts

### **Part 1: Data Preprocessing (10 points)**
- Handle missing values, outliers, normalization, and encoding.
- Generate correlation plots and feature visualizations.
- Save cleaned datasets as:


### **Part 2: Logistic Regression (40 points)**
- Implement logistic regression from scratch using **gradient descent** and the **sigmoid** function.
- Train on the *Penguins dataset* for binary classification.
- Target Accuracy: ‚â• 64%.
- Plot **loss vs. iterations**.
- Save model weights to:


### **Part 3: Linear Regression (25 points)**
- Implement **Ordinary Least Squares (OLS)** using:
\[
w = (X^T X)^{-1} X^T y
\]
- Evaluate using **Mean Squared Error (MSE)** and scatter plots.

### **Part 4: Ridge Regression (25 points)**
- Extend OLS by adding **L2 regularization**:
\[
w = (X^T X + ŒªI)^{-1} X^T y
\]
- Compare with linear regression and visualize Œª vs. MSE.

### **Bonus (+10 points)**
- Implement **Gradient Descent for Ridge Regression**.  
- Implement **Elastic Net Regularization** (L1 + L2).

---

## üß™ How to Run
```bash
# 1Ô∏è‚É£ Clone the repository
git clone https://github.com/chanduchallagonda/Pintos-os-project.git
cd Pintos-os-project

# 2Ô∏è‚É£ Run preprocessing notebook
jupyter notebook notebooks/part_1_preprocessing.ipynb

# 3Ô∏è‚É£ Train Logistic Regression
jupyter notebook notebooks/part_2_logistic_regression.ipynb

# 4Ô∏è‚É£ Run Linear and Ridge Regression
jupyter notebook notebooks/part_3_4_linear_ridge_regression.ipynb

